{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library","metadata":{"id":"XQuUKAHnPGqb"}},{"cell_type":"code","source":"! pip install wandb opencv-python-headless==4.1.2.30 albumentations torch-summary timm==0.5.4 einops joblib icecream  -qq -U","metadata":{"id":"EP8Uk0lzPGqi","execution":{"iopub.status.busy":"2022-02-02T14:24:02.482965Z","iopub.execute_input":"2022-02-02T14:24:02.483302Z","iopub.status.idle":"2022-02-02T14:24:23.59729Z","shell.execute_reply.started":"2022-02-02T14:24:02.483221Z","shell.execute_reply":"2022-02-02T14:24:23.595989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom glob import glob\nimport pathlib\nfrom pathlib import Path\nfrom torchsummary import summary\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nfrom pprint import pprint\nimport urllib.request\nimport csv\nimport numpy as np\nfrom einops import rearrange, reduce, repeat\nfrom torch.cuda import amp\nfrom tqdm import tqdm\nimport wandb\nimport time\nimport copy\nfrom collections import defaultdict\nfrom sklearn.metrics import mean_squared_error\nimport joblib\nimport gc\nimport os\nfrom icecream import ic\nfrom sklearn.model_selection import train_test_split\nimport gc\nimport cv2\nimport copy\nimport time\nimport random\nfrom PIL import Image\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\n# Utils\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport timm\n\nimport json\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\n\nfrom sklearn.metrics import f1_score","metadata":{"id":"V8YpfugSPGqk","execution":{"iopub.status.busy":"2022-02-02T14:24:23.601372Z","iopub.execute_input":"2022-02-02T14:24:23.602071Z","iopub.status.idle":"2022-02-02T14:24:28.553975Z","shell.execute_reply.started":"2022-02-02T14:24:23.602036Z","shell.execute_reply":"2022-02-02T14:24:28.552884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ENV","metadata":{"id":"EVCBodi-PGql"}},{"cell_type":"code","source":"\n# ENV = 'COLAB'\nENV = 'KAGGLE'\n# ENV = 'SYSTEM'\n\n# Option for Mixed Precision\n# FP16 = True\nFP16 = False\n\n\nCONFIG = dict(\n    nickname='b4noisy512 finecutmix',\n    seed=42,\n    backbone='tf_efficientnet_b4_ns',\n    embedder=None,\n    train_batch_size=8,\n    valid_batch_size=16,\n    img_size=512,\n    num_epochs=50,\n    early_stopping=False,\n    early_stopping_step=5,\n    learning_rate=1e-4,\n    scheduler='CosineAnnealingLR',\n    min_lr=1e-6,\n    T_max=100,\n    num_classes=25,\n    weight_decay=1e-6,\n    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    competition='lg',\n    _wandb_kernel='deb'\n)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"id":"fbOZOG8UPGqm","execution":{"iopub.status.busy":"2022-02-02T14:24:28.55647Z","iopub.execute_input":"2022-02-02T14:24:28.557101Z","iopub.status.idle":"2022-02-02T14:24:28.611349Z","shell.execute_reply.started":"2022-02-02T14:24:28.557053Z","shell.execute_reply":"2022-02-02T14:24:28.610122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SET SEED ","metadata":{"id":"M6BJH0gaPGqn"}},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n\nset_seed(CONFIG['seed'])\n","metadata":{"id":"Vj7acIAIPGqn","execution":{"iopub.status.busy":"2022-02-02T14:24:28.614092Z","iopub.execute_input":"2022-02-02T14:24:28.614426Z","iopub.status.idle":"2022-02-02T14:24:28.634804Z","shell.execute_reply.started":"2022-02-02T14:24:28.614385Z","shell.execute_reply":"2022-02-02T14:24:28.63381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read the Data\n","metadata":{"id":"0KA00McmPGqo"}},{"cell_type":"code","source":"import wandb\nrun = wandb.init(project=\"lg\",\n                 entity=\"jiwon7258\",\n                 config=CONFIG,\n                 job_type='train',\n                #  id='3199pcx4',\n                #  resume='must',\n                 )\ndataset = wandb.run.use_artifact(\n    'jiwon7258/lg/lg_train:v0', type='dataset')\n\nrun.name = CONFIG['nickname']\n\n# Download the artifact's contents\ndataset_dir = dataset.download()\ndataset_dir = Path(dataset_dir)\n","metadata":{"id":"jKC4FDfyPGqp","outputId":"d5d2b7aa-2d1f-4d40-c737-cd49ec3e84a7","execution":{"iopub.status.busy":"2022-02-02T14:24:28.639175Z","iopub.execute_input":"2022-02-02T14:24:28.639411Z","iopub.status.idle":"2022-02-02T14:28:54.10409Z","shell.execute_reply.started":"2022-02-02T14:24:28.639382Z","shell.execute_reply":"2022-02-02T14:28:54.103105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = dataset_dir\n# TEST_PATH = dataset_dir / 'test'","metadata":{"id":"ca-5DetEPGqq","execution":{"iopub.status.busy":"2022-02-02T14:28:54.105684Z","iopub.execute_input":"2022-02-02T14:28:54.106252Z","iopub.status.idle":"2022-02-02T14:28:54.782796Z","shell.execute_reply.started":"2022-02-02T14:28:54.106207Z","shell.execute_reply":"2022-02-02T14:28:54.781901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentations","metadata":{"id":"n-8x5P--PGqr"}},{"cell_type":"code","source":"data_transforms = {\n    \"train\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1,\n                           rotate_limit=90, p=0.5),\n        A.RGBShift(r_shift_limit=15, g_shift_limit=15,\n                   b_shift_limit=15, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Normalize(),\n        ToTensorV2()], p=1.),\n\n    \"valid\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(),\n        ToTensorV2()], p=1.)\n}\n","metadata":{"id":"Qetw9jGjPGqr","execution":{"iopub.status.busy":"2022-02-02T14:28:54.784576Z","iopub.execute_input":"2022-02-02T14:28:54.784899Z","iopub.status.idle":"2022-02-02T14:28:55.48086Z","shell.execute_reply.started":"2022-02-02T14:28:54.784838Z","shell.execute_reply":"2022-02-02T14:28:55.479891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"lF_SM2DGPGqs"}},{"cell_type":"code","source":"train_csv = sorted(glob(str(TRAIN_PATH / '*/*.csv')))\ntrain_jpg = sorted(glob(str(TRAIN_PATH / '*/*.jpg')))\ntrain_json = sorted(glob(str(TRAIN_PATH / '*/*.json')))\n\n\ncrops = []\ndiseases = []\nrisks = []\nlabels = []\n\nfor i in range(len(train_json)):\n    with open(train_json[i], 'r') as f:\n        sample = json.load(f)\n        crop = sample['annotations']['crop']\n        disease = sample['annotations']['disease']\n        risk = sample['annotations']['risk']\n        label=f\"{crop}_{disease}_{risk}\"\n    \n        crops.append(crop)\n        diseases.append(disease)\n        risks.append(risk)\n        labels.append(label)\n        \nlabel_unique = sorted(np.unique(labels))\nlabel_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n\ntrain_labels = [label_unique[k] for k in labels] # len = train_len","metadata":{"id":"XW2BnvG6PGqs","execution":{"iopub.status.busy":"2022-02-02T14:28:55.482368Z","iopub.execute_input":"2022-02-02T14:28:55.482704Z","iopub.status.idle":"2022-02-02T14:28:57.276785Z","shell.execute_reply.started":"2022-02-02T14:28:55.482657Z","shell.execute_reply":"2022-02-02T14:28:57.275813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_jpg = np.array(train_jpg)\ntrain_labels = np.array(train_labels)","metadata":{"id":"AIQNMAQ_PGqt","execution":{"iopub.status.busy":"2022-02-02T14:28:57.278313Z","iopub.execute_input":"2022-02-02T14:28:57.278671Z","iopub.status.idle":"2022-02-02T14:28:58.04508Z","shell.execute_reply.started":"2022-02-02T14:28:57.278625Z","shell.execute_reply":"2022-02-02T14:28:58.04409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, train_img, train_label, transforms=None):\n        self.imgs = train_img\n        self.labels = train_label\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.imgs)\n    \n    def __getitem__(self, index):\n        img_path = self.imgs[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        target = self.labels[index]\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n            \n        return img, target\n    \n# trainDataset = CustomDataset(X_train, y_train, transforms = data_transforms['train'])\n# trainDataloader = DataLoader(\n#     trainDataset, batch_size=CONFIG['train_batch_size'], shuffle=True)\n\n# validDataset = CustomDataset(X_val, y_val, transforms = data_transforms['valid'])\n# validDataloader = DataLoader(validDataset, batch_size = CONFIG['valid_batch_size'], shuffle = True)","metadata":{"id":"40-6mAVePGqt","execution":{"iopub.status.busy":"2022-02-02T14:28:58.049789Z","iopub.execute_input":"2022-02-02T14:28:58.050138Z","iopub.status.idle":"2022-02-02T14:28:58.716603Z","shell.execute_reply.started":"2022-02-02T14:28:58.05009Z","shell.execute_reply":"2022-02-02T14:28:58.715511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\ntrain_datasets = []\nvalid_datasets = []\ntrain_dataloaders = []\nvalid_dataloaders = []\n\nskf = StratifiedKFold(n_splits = 5)\n\nfor step, (train_index, val_index) in enumerate(skf.split(X = train_jpg, y= train_labels)):\n    X_train = train_jpg[train_index]\n    y_train = train_labels[train_index]\n    X_val = train_jpg[val_index]\n    y_val = train_labels[val_index]\n    train_datasets.append(CustomDataset(\n        X_train, y_train, transforms=data_transforms['train']))\n    valid_datasets.append(CustomDataset(\n        X_val, y_val, transforms=data_transforms['valid']))\n    train_dataloaders.append(DataLoader(\n        train_datasets[step], batch_size=CONFIG['train_batch_size'], shuffle=True)\n    )\n    valid_dataloaders.append(\n        DataLoader(\n            valid_datasets[step], batch_size=CONFIG['valid_batch_size'], shuffle=True)\n    )\n","metadata":{"id":"TucH1uf_PGqu","execution":{"iopub.status.busy":"2022-02-02T14:28:58.718387Z","iopub.execute_input":"2022-02-02T14:28:58.721956Z","iopub.status.idle":"2022-02-02T14:28:59.502248Z","shell.execute_reply.started":"2022-02-02T14:28:58.721914Z","shell.execute_reply":"2022-02-02T14:28:59.501334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"qquyNic-PGqu"}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, backbone, embedder, pretrained=True):\n        super(Model, self).__init__()\n        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n        self.backbone.reset_classifier(0)\n        self.n_features = self.backbone.num_features\n        self.fc = nn.Linear(self.n_features, CONFIG['num_classes'])\n\n    def forward(self, images):\n        # features = (bs, embedding_size)\n        features = self.backbone(images)\n        # outputs  = (bs, num_classes)\n        # if isinstance(features, tuple):\n        #   features = features[0]\n        output = self.fc(features)\n        return output\n\n\nmodel = Model(CONFIG['backbone'], CONFIG['embedder'], pretrained = True)\nmodel.to(CONFIG['device'])\n;","metadata":{"id":"EeRw9VshPGqv","outputId":"414ee157-a230-49c6-8ca2-c3d612b4daa1","execution":{"iopub.status.busy":"2022-02-02T14:28:59.506312Z","iopub.execute_input":"2022-02-02T14:28:59.507277Z","iopub.status.idle":"2022-02-02T14:29:07.204096Z","shell.execute_reply.started":"2022-02-02T14:28:59.507232Z","shell.execute_reply":"2022-02-02T14:29:07.203118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(\n    params=model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n","metadata":{"id":"5yml5ZPPPGqv","execution":{"iopub.status.busy":"2022-02-02T14:29:07.208158Z","iopub.execute_input":"2022-02-02T14:29:07.209099Z","iopub.status.idle":"2022-02-02T14:29:07.899059Z","shell.execute_reply.started":"2022-02-02T14:29:07.209058Z","shell.execute_reply":"2022-02-02T14:29:07.898011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def criterion(logits: torch.tensor, targets: torch.tensor):\n    return nn.CrossEntropyLoss()(logits.view(-1,CONFIG['num_classes']), targets.view(-1))","metadata":{"id":"iIKSttnGPGqv","execution":{"iopub.status.busy":"2022-02-02T14:29:07.901107Z","iopub.execute_input":"2022-02-02T14:29:07.901818Z","iopub.status.idle":"2022-02-02T14:29:08.562083Z","shell.execute_reply.started":"2022-02-02T14:29:07.90176Z","shell.execute_reply":"2022-02-02T14:29:08.561127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CutMix","metadata":{"id":"HPYus03_UfvH"}},{"cell_type":"code","source":"def cutmix(img, target):\n    \"\"\" \n    img : (bs, C, H, W)\n    target\n        - (bs,)\n        - integer scalar\n    \"\"\"\n    batch_size, C, H, W, = img.shape\n    # ic(img.shape)\n\n    img_a = img\n    target_a = target\n    img_b = img\n    target_b = target\n\n    mask = np.arange(batch_size)\n    mask = np.random.permutation(mask)\n    # ic(mask)\n    img_b = img_a[mask]\n    target_b = target_a[mask]\n    # ic(target, target_b)\n\n    lam = np.random.uniform(low=0.3, high=0.7)\n    r_x = np.random.uniform(low=0, high=W)\n    r_y = np.random.uniform(low=0, high=H)\n    r_w = W * np.sqrt(1 - lam)\n    r_h = H * np.sqrt(1 - lam)\n    ic(lam, r_x, r_y, r_w, r_h)\n    x1 = np.int(np.clip((r_x - r_w) / 2, 0, W))\n    x2 = np.int(np.clip((r_x + r_w) / 2, 0, W))\n    y1 = np.int(np.clip((r_y - r_h) / 2, 0, H))\n    y2 = np.int(np.clip((r_y + r_h) / 2, 0, H))\n    ic(x1, x2, y1, y2)\n\n    img_a[:, :, y1:y2, x1:x2] = img_b[:, :, y1:y2, x1:x2]\n\n    # Adjust lambda to exact ratio\n\n    lam = 1 - (x2 - x1) * (y2 - y1) / float(W * H)\n\n    return img_a, target_b, lam\n","metadata":{"id":"l8XzN0C2UfvH","execution":{"iopub.status.busy":"2022-02-02T14:29:08.564603Z","iopub.execute_input":"2022-02-02T14:29:08.564948Z","iopub.status.idle":"2022-02-02T14:29:09.297501Z","shell.execute_reply.started":"2022-02-02T14:29:08.564901Z","shell.execute_reply":"2022-02-02T14:29:09.296471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"   # Training Function","metadata":{"id":"cTfsj-Y17H0C"}},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    # train 모드로 변경\n    model.train()\n\n    # for the Mixed Precision\n    # Pytorch 예제 : https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples\n    if(FP16):\n        scaler = amp.GradScaler()\n\n    losses = AverageMeter()\n    accuracy = AverageMeter()\n    f1 = AverageMeter()\n\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n\n    for step, (img, target) in bar:\n        img, target_b, lam = cutmix(img, target)\n\n        img = img.to(device)\n        target = target.to(device)\n        target_b = target_b.to(device)\n\n        batch_size = img.shape[0]\n\n        if(FP16):\n            with amp.autocast(enabled=True):\n                logits = model(img)\n                loss = criterion(logits, target) * lam + \\\n                    criterion(logits, target_b) * (1-lam)\n\n                # loss를 Scale\n                # Scaled Grdients를 계산(call)하기 위해 scaled loss를 backward()\n                scaler.scale(loss).backward()\n                # scaler.step() first unscales the gradients of the optimizer's assigned params.\n                # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n                # otherwise, optimizer.step() is skipped.\n                scaler.step(optimizer)\n\n                # Updates the scale for next iteration.\n                scaler.update()\n\n        else:\n            logits = model(img)\n            loss = criterion(logits, target) * lam + \\\n                criterion(logits, target_b) * (1-lam)\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n            optimizer.step()\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # change learning rate by Scheduler\n        if scheduler is not None:\n            scheduler.step()\n\n        # loss.item()은 loss를 Python Float으로 반환\n        losses.update(loss.item())\n\n        # logits\n        logits = logits.detach().cpu()\n\n        # acc, f1\n        probs = torch.softmax(logits, dim = -1)\n        output = np.argmax(probs, axis=-1)\n        output_b = np.argsort(probs)[:,-2]\n        if (lam >= 0.5):\n            step_acc = np.mean(\n                output.view(-1).numpy() == target.view(-1).detach().cpu().numpy())\n            step_f1 = f1_score(output.view(-1).numpy(),\n                               target.view(-1).detach().cpu().numpy(), average='macro')\n            step_acc_b = np.mean(\n                output_b.view(-1).numpy() == target_b.view(-1).detach().cpu().numpy())\n            step_f1_b = f1_score(output_b.view(-1).numpy(),\n                                 target_b.view(-1).detach().cpu().numpy(), average='macro')\n        else:\n            step_acc = np.mean(\n                output.view(-1).numpy() == target_b.view(-1).detach().cpu().numpy())\n            step_f1 = f1_score(output.view(-1).numpy(),\n                               target_b.view(-1).detach().cpu().numpy(), average='macro')\n            step_acc_b = np.mean(\n                output_b.view(-1).numpy() == target.view(-1).detach().cpu().numpy())\n            step_f1_b = f1_score(output_b.view(-1).numpy(),\n                                 target.view(-1).detach().cpu().numpy(), average='macro')\n\n                                 \n        step_acc = step_acc * lam + step_acc_b * (1-lam)\n        step_f1 = step_f1 * lam + step_f1_b * (1-lam)\n\n        accuracy.update(step_acc)\n        f1.update(step_f1)\n\n        # loss\n        train_loss = losses.avg\n        train_acc = accuracy.avg\n        train_f1 = f1.avg\n\n        bar.set_postfix(\n            Epoch=epoch, Train_Loss=train_loss, LR=optimizer.param_groups[\n                0][\"lr\"], accuracy=train_acc, f1=train_f1\n        )\n\n    # Garbage Collector\n    gc.collect()\n\n    return losses.avg, accuracy.avg, f1.avg\n","metadata":{"id":"iCswmFhy7H0D","execution":{"iopub.status.busy":"2022-02-02T14:29:09.299337Z","iopub.execute_input":"2022-02-02T14:29:09.299852Z","iopub.status.idle":"2022-02-02T14:29:10.004276Z","shell.execute_reply.started":"2022-02-02T14:29:09.299806Z","shell.execute_reply":"2022-02-02T14:29:10.003282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"   # Validation Function","metadata":{"id":"C5PB19nv7H0D"}},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n\n    losses = AverageMeter()\n    accuracy = AverageMeter()\n    f1 = AverageMeter()\n\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n\n    for step, (img, target) in bar:\n        img = img.to(device)\n        target = target.to(device)\n\n        batch_size = img.shape[0]\n\n        logits = model(img)\n        loss = criterion(logits, target)\n\n        # loss.item()은 loss를 Python Float으로 반환\n        losses.update(loss.item())\n\n        # logits\n        logits = logits.detach().cpu()\n\n        output = np.argmax(torch.softmax(logits, dim=-1), axis=-1)\n        step_acc = np.mean(\n            output.view(-1).numpy() == target.view(-1).detach().cpu().numpy())\n        step_f1 = f1_score(output.view(-1).numpy(),\n                           target.view(-1).detach().cpu().numpy(), average='macro')\n\n        accuracy.update(step_acc)\n        f1.update(step_f1)\n\n        # loss\n        val_loss = losses.avg\n        val_acc = accuracy.avg\n        val_f1 = f1.avg\n\n        bar.set_postfix(\n            Epoch=epoch, Valid_Loss=val_loss, LR=optimizer.param_groups[\n                0][\"lr\"], accuracy=val_acc, f1=val_f1\n        )\n\n    gc.collect()\n\n    return losses.avg, accuracy.avg, f1.avg\n","metadata":{"id":"aJRJrmiG7H0E","execution":{"iopub.status.busy":"2022-02-02T14:29:10.009227Z","iopub.execute_input":"2022-02-02T14:29:10.009612Z","iopub.status.idle":"2022-02-02T14:29:10.767853Z","shell.execute_reply.started":"2022-02-02T14:29:10.009562Z","shell.execute_reply":"2022-02-02T14:29:10.766825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef run_training(\n    model,\n    optimizer,\n    scheduler,\n    device,\n    num_epochs,\n    metric_prefix=\"\",\n    file_prefix=\"\",\n    early_stopping=True,\n    early_stopping_step=10,\n    START_EPOCH=0,\n):\n    # To automatically log graidents\n    wandb.watch(model, log_freq=100)\n\n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU:{}\\n\".format(torch.cuda.get_device_name()))\n\n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = np.inf\n    history = defaultdict(list)\n    early_stop_counter = 0\n\n    # num_epochs만큼, train과 val을 실행한다\n    for epoch in range(START_EPOCH, START_EPOCH + num_epochs):\n        gc.collect()\n\n        fold_num = 5\n        fold = epoch % fold_num\n\n        # for fold in range(fold_num) :\n\n        trainDataloader = train_dataloaders[fold]\n        validDataloader = valid_dataloaders[fold]\n\n        train_train_loss, train_accuracy, train_f1 = train_one_epoch(\n            model,\n            optimizer,\n            scheduler,\n            dataloader=trainDataloader,\n            device=device,\n            epoch=epoch,\n        )\n\n        val_loss, val_accuracy, val_f1 = valid_one_epoch(\n            model, validDataloader, device=device, epoch=epoch\n        )\n\n        history[f\"{metric_prefix}Train Loss\"].append(train_train_loss)\n        history[f\"{metric_prefix}Train Accuracy\"].append(train_accuracy)\n        history[f\"{metric_prefix}Train F1\"].append(train_f1)\n        history[f\"{metric_prefix}Valid Loss\"].append(val_loss)\n        history[f\"{metric_prefix}Valid Accuracy\"].append(val_accuracy)\n        history[f\"{metric_prefix}Valid F1\"].append(val_f1)\n\n        # Log the metrics\n        wandb.log(\n            {\n                f\"{metric_prefix}Train Loss\": train_train_loss,\n                f\"{metric_prefix}Valid Loss\": val_loss,\n                f\"{metric_prefix}Train Accuracy\": train_accuracy,\n                f\"{metric_prefix}Valid Accuracy\": val_accuracy,\n                f\"{metric_prefix}Train F1\": train_f1,\n                f\"{metric_prefix}Valid F1\": val_f1,\n            }\n        )\n\n        print(f\"Valid Loss : {val_loss}\")\n\n        torch.save(model.state_dict(), f'{CONFIG[\"nickname\"]}last.bin')\n        wandb.save(f'{CONFIG[\"nickname\"]}last.bin')\n\n        # deep copy the model\n        if val_loss <= best_loss:\n            early_stop_counter = 0\n\n            print(\n                f\"Validation Loss improved( {best_loss} ---> {val_loss}  )\"\n            )\n\n            # Update Best Loss\n            best_loss = val_loss\n\n            # Update Best Model Weight\n            # run.summary['Best RMSE'] = best_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n\n            PATH = \"{}epoch{:.0f}_Loss{:.4f}.bin\".format(\n                file_prefix, epoch, best_loss)\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            wandb.save(PATH)\n\n            print(f\"Model Saved\")\n\n        elif early_stopping:\n            early_stop_counter += 1\n            if early_stop_counter > early_stopping_step:\n                break\n\n        START_EPOCH = epoch + 1\n        # break\n\n    end = time.time()\n    time_elapsed = end - start\n    print(\n        \"Training complete in {:.0f}h {:.0f}m {:.0f}s\".format(\n            time_elapsed // 3600,\n            (time_elapsed % 3600) // 60,\n            (time_elapsed % 3600) % 60,\n        )\n    )\n    print(\"Best Loss: {:.4f}\".format(best_loss))\n\n    return model, history\n","metadata":{"id":"kZfD8q3s7H0E","execution":{"iopub.status.busy":"2022-02-02T14:29:10.77521Z","iopub.execute_input":"2022-02-02T14:29:10.778233Z","iopub.status.idle":"2022-02-02T14:29:11.569057Z","shell.execute_reply.started":"2022-02-02T14:29:10.778187Z","shell.execute_reply":"2022-02-02T14:29:11.567741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = 'epoch71_Loss0.0003.bin'\nRUN_PATH = 'jiwon7258/lg/1v3ffaqb'\nwandb.restore(MODEL_NAME, RUN_PATH, root='./')\nmodel.load_state_dict(torch.load(\n    MODEL_NAME, map_location=CONFIG['device']))\n","metadata":{"id":"eX2s0GMOPGqy","outputId":"9dca4af5-3d1c-42b1-eba1-1c9ddf70b59e","execution":{"iopub.status.busy":"2022-02-02T14:29:11.570952Z","iopub.execute_input":"2022-02-02T14:29:11.571263Z","iopub.status.idle":"2022-02-02T14:29:17.368366Z","shell.execute_reply.started":"2022-02-02T14:29:11.571211Z","shell.execute_reply":"2022-02-02T14:29:17.367399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fast Start : 5 epoch","metadata":{"id":"AdVDFxn6PGqy"}},{"cell_type":"code","source":"for param in model.backbone.named_parameters():\n    param[1].requires_grad = False","metadata":{"id":"vMfEjYQFPGqz","execution":{"iopub.status.busy":"2022-02-02T14:29:17.370135Z","iopub.execute_input":"2022-02-02T14:29:17.370423Z","iopub.status.idle":"2022-02-02T14:29:18.04452Z","shell.execute_reply.started":"2022-02-02T14:29:17.370383Z","shell.execute_reply":"2022-02-02T14:29:18.043457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ic.disable()\nrun_training(\n    model=model,\n    optimizer=optimizer,\n    scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer=optimizer, T_max=CONFIG['T_max'], eta_min=CONFIG['min_lr']),\n    device=device,\n    num_epochs=5,\n    metric_prefix=\"\",\n    file_prefix=\"\",\n    early_stopping=CONFIG['early_stopping'],\n    early_stopping_step=CONFIG['early_stopping_step'],\n    START_EPOCH=0,\n);\n","metadata":{"id":"DEZO7sK4PGqz","outputId":"f844e193-ac58-4d69-9bb0-225c3f1ff3fa","execution":{"iopub.status.busy":"2022-02-02T14:29:18.049597Z","iopub.execute_input":"2022-02-02T14:29:18.049875Z","iopub.status.idle":"2022-02-02T14:46:50.109874Z","shell.execute_reply.started":"2022-02-02T14:29:18.049811Z","shell.execute_reply":"2022-02-02T14:46:50.108892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 400 epochs","metadata":{"id":"pDi14R3uPGqz"}},{"cell_type":"code","source":"optimizer = torch.optim.Adam(\n    params=model.parameters(), lr=5e-6, weight_decay=CONFIG['weight_decay'])","metadata":{"id":"L5G3qhmFPGqz","execution":{"iopub.status.busy":"2022-02-02T14:46:50.112186Z","iopub.execute_input":"2022-02-02T14:46:50.112415Z","iopub.status.idle":"2022-02-02T14:46:50.868489Z","shell.execute_reply.started":"2022-02-02T14:46:50.112384Z","shell.execute_reply":"2022-02-02T14:46:50.867372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param in model.backbone.named_parameters():\n    param[1].requires_grad = True\n","metadata":{"id":"Ae1QgOXSPGqz","execution":{"iopub.status.busy":"2022-02-02T14:46:50.873717Z","iopub.execute_input":"2022-02-02T14:46:50.874447Z","iopub.status.idle":"2022-02-02T14:46:51.612767Z","shell.execute_reply.started":"2022-02-02T14:46:50.874414Z","shell.execute_reply":"2022-02-02T14:46:51.611787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ic.disable()\nrun_training(\n    model=model,\n    optimizer=optimizer,\n    scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer=optimizer, T_max=CONFIG['T_max'], eta_min=CONFIG['min_lr']),\n    # scheduler=None,\n    device=device,\n    num_epochs=400,\n    metric_prefix=\"\",\n    file_prefix=\"\",\n    early_stopping=CONFIG['early_stopping'],\n    early_stopping_step=CONFIG['early_stopping_step'],\n    START_EPOCH=5,\n)\n","metadata":{"id":"dKXcxeLEPGq0","execution":{"iopub.status.busy":"2022-02-02T14:46:51.616379Z","iopub.execute_input":"2022-02-02T14:46:51.616709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"id":"_lSY1fu15fPu","trusted":true},"execution_count":null,"outputs":[]}]}